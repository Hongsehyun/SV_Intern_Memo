{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "sK8v8y9xwPr7",
        "outputId": "06cdb45e-517c-413c-a552-7af878a5797d"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8hXrPBLYwhUK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "from resnet import resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z47anIEfy4qG"
      },
      "outputs": [],
      "source": [
        "def set_random_seeds(random_seed=0):\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(random_seed)\n",
        "    random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "o-qANuoizpXe"
      },
      "outputs": [],
      "source": [
        "def prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256):\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    train_set = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True, transform=train_transform) \n",
        "    # We will use test set for validation and test in this project.\n",
        "    # Do not use test set for validation in practice!\n",
        "    test_set = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True, transform=test_transform)\n",
        "\n",
        "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
        "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset=train_set, batch_size=train_batch_size,\n",
        "        sampler=train_sampler, num_workers=num_workers)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        dataset=test_set, batch_size=eval_batch_size,\n",
        "        sampler=test_sampler, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rmElTWvHz_iu"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, test_loader, device, learning_rate=1e-1, num_epochs=20):\n",
        "\n",
        "    # The training configurations were not carefully selected.\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10.\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
        "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1, last_epoch=-1)\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    # eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "    eval_loss, eval_accuracy, len_test_dataset = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "    eval_accuracy_int = eval_accuracy.item()\n",
        "\n",
        "    # print(\"Epoch: {:02d} Eval Loss: {:.3f} Eval Acc: {:.3f} length of Dataset: {}\".format(-1, eval_loss, eval_accuracy_int, len_test_dataset))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        # train_loss_int = running_loss // len(train_loader.dataset)\n",
        "        # train_accuracy_int = running_corrects // len(train_loader.dataset)\n",
        "        train_accuracy_int_item = running_corrects.item()\n",
        "        \n",
        "        train_loss = running_loss/len(train_loader.dataset)\n",
        "        train_accuracy = train_accuracy_int_item/len(train_loader.dataset)\n",
        "        \n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy, len_test_dataset = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "        eval_accuracy_int = eval_accuracy.item()\n",
        "        \n",
        "        eval_loss_value = eval_loss/len_test_dataset\n",
        "        eval_accuracy_value = eval_accuracy_int/len_test_dataset\n",
        "\n",
        "        # Set learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        print(\"Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f} length of Dataset: {}\".format(epoch, train_loss, train_accuracy, eval_loss_value, eval_accuracy_value, len_test_dataset))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ss7VE4e_ztZ-"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device, criterion=None):\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    running_loss = 0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        if criterion is not None:\n",
        "            loss = criterion(outputs, labels).item()\n",
        "        else:\n",
        "            loss = 0\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    # eval_loss = running_loss / len(test_loader.dataset)\n",
        "    # eval_accuracy = running_corrects / len(test_loader.dataset)\n",
        "    # return eval_loss, eval_accuracy\n",
        "    \n",
        "    eval_loss = running_loss\n",
        "    eval_accuracy = running_corrects\n",
        "    return eval_loss, eval_accuracy, len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zE90jLxz0Eyd"
      },
      "outputs": [],
      "source": [
        "def calibrate_model(model, loader, device=torch.device(\"cpu:0\")):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        _ = model(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XK-eAkq50Kkr"
      },
      "outputs": [],
      "source": [
        "def measure_inference_latency(model,\n",
        "                              device,\n",
        "                              input_size=(1, 3, 32, 32),\n",
        "                              num_samples=100,\n",
        "                              num_warmups=10):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    x = torch.rand(size=input_size).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_warmups):\n",
        "            _ = model(x)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        for _ in range(num_samples):\n",
        "            _ = model(x)\n",
        "            torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_time_ave = elapsed_time / num_samples\n",
        "\n",
        "    return elapsed_time_ave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "plX48HtM0Plb"
      },
      "outputs": [],
      "source": [
        "def save_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.save(model.state_dict(), model_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Trd3EQfT0Sr1"
      },
      "outputs": [],
      "source": [
        "def load_model(model, model_filepath, device):\n",
        "\n",
        "    model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hAlqDSCW0VM0"
      },
      "outputs": [],
      "source": [
        "def save_torchscript_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
        "\n",
        "def load_torchscript_model(model_filepath, device):\n",
        "\n",
        "    model = torch.jit.load(model_filepath, map_location=device)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_xFHNVjI0YnO"
      },
      "outputs": [],
      "source": [
        "def create_model(num_classes=10):\n",
        "\n",
        "    # The number of channels in ResNet18 is divisible by 8.\n",
        "    # This is required for fast GEMM integer matrix multiplication.\n",
        "    # model = torchvision.models.resnet18(pretrained=False)\n",
        "    model = resnet18(num_classes=num_classes, pretrained=False)\n",
        "\n",
        "    # We would use the pretrained ResNet18 as a feature extractor.\n",
        "    # for param in model.parameters():\n",
        "    #     param.requires_grad = False\n",
        "    \n",
        "    # Modify the last FC layer\n",
        "    # num_features = model.fc.in_features\n",
        "    # model.fc = nn.Linear(num_features, 10)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "K4bhShxg0wMW"
      },
      "outputs": [],
      "source": [
        "class QuantizedResNet18(nn.Module):\n",
        "    def __init__(self, model_fp32):\n",
        "        super(QuantizedResNet18, self).__init__()\n",
        "        # QuantStub converts tensors from floating point to quantized.\n",
        "        # This will only be used for inputs.\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        # DeQuantStub converts tensors from quantized to floating point.\n",
        "        # This will only be used for outputs.\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "        # FP32 model\n",
        "        self.model_fp32 = model_fp32\n",
        "\n",
        "    def forward(self, x):\n",
        "        # manually specify where tensors will be converted from floating\n",
        "        # point to quantized in the quantized model\n",
        "        x = self.quant(x)\n",
        "        x = self.model_fp32(x)\n",
        "        # manually specify where tensors will be converted from quantized\n",
        "        # to floating point in the quantized model\n",
        "        x = self.dequant(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1x96aROe001d"
      },
      "outputs": [],
      "source": [
        "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32)):\n",
        "\n",
        "    model_1.to(device)\n",
        "    model_2.to(device)\n",
        "\n",
        "    for _ in range(num_tests):\n",
        "        x = torch.rand(size=input_size).to(device)\n",
        "        y1 = model_1(x).detach().cpu().numpy()\n",
        "        y2 = model_2(x).detach().cpu().numpy()\n",
        "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
        "            print(\"Model equivalence test sample failed: \")\n",
        "            print(y1)\n",
        "            print(y2)\n",
        "            return False\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ezvvmiEK040i"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "    random_seed = 0\n",
        "    num_classes = 10\n",
        "    cuda_device = torch.device(\"cuda:0\")\n",
        "    cpu_device = torch.device(\"cpu:0\")\n",
        "\n",
        "    model_dir = \"saved_models\"\n",
        "    model_filename = \"resnet18_cifar10.pt\"\n",
        "    quantized_model_filename = \"resnet18_quantized_cifar10.pt\"\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)\n",
        "\n",
        "    set_random_seeds(random_seed=random_seed)\n",
        "\n",
        "    # Create an untrained model.\n",
        "    model = create_model(num_classes=num_classes)\n",
        "\n",
        "    train_loader, test_loader = prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256)\n",
        "    \n",
        "    \n",
        "    #   ********************   Model Training Part   ********************   #\n",
        "    #   ********************   Model Training Part   ********************   #\n",
        "    #   ********************   Model Training Part   ********************   #\n",
        "    print(\"Training Model...\")\n",
        "    model = train_model(model=model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-1, num_epochs=100)\n",
        "\n",
        "    # Save model.\n",
        "    save_model(model=model, model_dir=model_dir, model_filename=model_filename)\n",
        "    torch.save(model, 'saved_models/resnet18_cifar10_Full.pth')\n",
        "    \n",
        "    #   ********************   Quantization Part   ********************   #\n",
        "    #   ********************   Quantization Part   ********************   #\n",
        "    #   ********************   Quantization Part   ********************   #\n",
        "    # Load a pretrained model.\n",
        "    model = load_model(model=model, model_filepath=model_filepath, device=cuda_device)\n",
        "    # Move the model to CPU since static quantization does not support CUDA currently.\n",
        "    model.to(cpu_device)\n",
        "    # Make a copy of the model for layer fusion\n",
        "    fused_model = copy.deepcopy(model)\n",
        "\n",
        "    model.eval()\n",
        "    print(\"model Architecture Printing...\")\n",
        "    print(model)\n",
        "    # The model has to be switched to training mode before any layer fusion.\n",
        "    # Otherwise the quantization aware training will not work correctly.\n",
        "    fused_model.eval()\n",
        "\n",
        "    # Fuse the model in place rather manually.\n",
        "    fused_model = torch.quantization.fuse_modules(fused_model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
        "    for module_name, module in fused_model.named_children():\n",
        "        if \"layer\" in module_name:\n",
        "            for basic_block_name, basic_block in module.named_children():\n",
        "                torch.quantization.fuse_modules(basic_block, [[\"conv1\", \"bn1\", \"relu\"], [\"conv2\", \"bn2\"]], inplace=True)\n",
        "                for sub_block_name, sub_block in basic_block.named_children():\n",
        "                    if sub_block_name == \"downsample\":\n",
        "                        torch.quantization.fuse_modules(sub_block, [[\"0\", \"1\"]], inplace=True)\n",
        "\n",
        "    # Print FP32 model.\n",
        "    print(\"Resnet Pretrained Model Printing...\")\n",
        "    print(model)\n",
        "    # Print fused model.\n",
        "    print(\"Resnet Pretrained and Fused Model Printing...\")\n",
        "    print(fused_model)\n",
        "\n",
        "    # Model and fused model should be equivalent.\n",
        "    model.eval()\n",
        "    fused_model.eval()\n",
        "    print(\"Checking Model Equivalence ...\")\n",
        "    assert model_equivalence(model_1=model, model_2=fused_model, device=cpu_device, rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)), \"Fused model is not equivalent to the original model!\"\n",
        "\n",
        "    # Prepare the model for quantization aware training. This inserts observers in\n",
        "    # the model that will observe activation tensors during calibration.\n",
        "    quantized_model = QuantizedResNet18(model_fp32=fused_model)\n",
        "    # Using un-fused model will fail.\n",
        "    # Because there is no quantized layer implementation for a single batch normalization layer.\n",
        "    # quantized_model = QuantizedResNet18(model_fp32=model)\n",
        "    # Select quantization schemes from \n",
        "    # https://pytorch.org/docs/stable/quantization-support.html\n",
        "    quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "    # Custom quantization configurations\n",
        "    # quantization_config = torch.quantization.default_qconfig\n",
        "    # quantization_config = torch.quantization.QConfig(activation=torch.quantization.MinMaxObserver.with_args(dtype=torch.quint8), weight=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
        "\n",
        "    quantized_model.qconfig = quantization_config\n",
        "    \n",
        "    # Print quantization configurations\n",
        "    print(\"Printing Quantization Configuration...\")\n",
        "    print(quantized_model.qconfig)\n",
        "\n",
        "    # https://pytorch.org/docs/stable/_modules/torch/quantization/quantize.html#prepare_qat\n",
        "    print(\"prepare Quantization Aware Training...\")\n",
        "    torch.quantization.prepare_qat(quantized_model, inplace=True)\n",
        "\n",
        "    # # Use training data for calibration.\n",
        "    print(\"Training QAT Model...\")\n",
        "    quantized_model.train()\n",
        "    #   ********************   Quantized Model Training Part   ********************   #\n",
        "    #   ********************   Quantized Model Training Part   ********************   #\n",
        "    #   ********************   Quantized Model Training Part   ********************   #\n",
        "    train_model(model=quantized_model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-3, num_epochs=10)\n",
        "    quantized_model.to(cpu_device)\n",
        "\n",
        "    # Using high-level static quantization wrapper\n",
        "    # The above steps, including torch.quantization.prepare, calibrate_model, and torch.quantization.convert, are also equivalent to\n",
        "    # quantized_model = torch.quantization.quantize_qat(model=quantized_model, run_fn=train_model, run_args=[train_loader, test_loader, cuda_device], mapping=None, inplace=False)\n",
        "\n",
        "    print(\"Completed Quantization Aware Training!\")\n",
        "    print(\"Convert Quantized Model...\")\n",
        "    quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
        "    quantized_model.eval()\n",
        "\n",
        "    # Print quantized model.\n",
        "    print(quantized_model)\n",
        "\n",
        "    # Save quantized model.\n",
        "    save_torchscript_model(model=quantized_model, model_dir=model_dir, model_filename=quantized_model_filename)\n",
        "\n",
        "    # Load quantized model.\n",
        "    quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)\n",
        "\n",
        "    print(\"\\n\\nevaluate Pretrained Model and QAT Model\")\n",
        "    _, fp32_eval_accuracy, len_test_dataset = evaluate_model(model=model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "    _, int8_eval_accuracy, len_test_dataset = evaluate_model(model=quantized_jit_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "    \n",
        "    fp32_eval_accuracy_item = fp32_eval_accuracy.item()\n",
        "    int8_eval_accuracy_item = int8_eval_accuracy.item()\n",
        "\n",
        "    fp32_eval_accuracy_value = fp32_eval_accuracy_item/len_test_dataset\n",
        "    int8_eval_accuracy_value = int8_eval_accuracy_item/len_test_dataset\n",
        "\n",
        "    # Skip this assertion since the values might deviate a lot.\n",
        "    # assert model_equivalence(model_1=model, model_2=quantized_jit_model, device=cpu_device, rtol=1e-01, atol=1e-02, num_tests=100, input_size=(1,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
        "\n",
        "    print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy_value))\n",
        "    print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy_value))\n",
        "\n",
        "    fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "    int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "    # int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "    # fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
        "    \n",
        "    print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
        "    # print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
        "    print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
        "    # print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6f3174f697ce4ffdaa400b5397175caf",
            "e25c33aebdd54a32ab372276da851a65",
            "17e90a5325d94f4881066712971e77bd",
            "8cad15043d024aa8930530311b50d54b",
            "227676e5ba1542d9848216dbecdfc1d7",
            "0ca7d112caef4dbe87dedf7627847e08",
            "e88bdbda8cce46f985fd7a2a28fa99db",
            "7c5bd5c9be36428bb379e76d2a4e7041",
            "d9726b7986894d5ab030657741cc72c8",
            "8f1a2c4dc3a341c184512e75ad4043f9",
            "f17a0fbc1d1c406da612c6364f375e9b"
          ]
        },
        "id": "GahJa6-d1QV1",
        "outputId": "d0cae2b6-adc5-4e28-a1f7-c565083eb697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Training Model...\n",
            "Epoch: 000 Train Loss: 1.847 Train Acc: 0.334 Eval Loss: 1.397 Eval Acc: 0.486 length of Dataset: 10000\n",
            "Epoch: 001 Train Loss: 1.284 Train Acc: 0.535 Eval Loss: 1.183 Eval Acc: 0.573 length of Dataset: 10000\n",
            "Epoch: 002 Train Loss: 1.052 Train Acc: 0.628 Eval Loss: 1.012 Eval Acc: 0.643 length of Dataset: 10000\n",
            "Epoch: 003 Train Loss: 0.898 Train Acc: 0.685 Eval Loss: 0.904 Eval Acc: 0.685 length of Dataset: 10000\n",
            "Epoch: 004 Train Loss: 0.798 Train Acc: 0.721 Eval Loss: 0.864 Eval Acc: 0.703 length of Dataset: 10000\n",
            "Epoch: 005 Train Loss: 0.708 Train Acc: 0.754 Eval Loss: 0.690 Eval Acc: 0.764 length of Dataset: 10000\n",
            "Epoch: 006 Train Loss: 0.633 Train Acc: 0.780 Eval Loss: 0.665 Eval Acc: 0.775 length of Dataset: 10000\n",
            "Epoch: 007 Train Loss: 0.575 Train Acc: 0.801 Eval Loss: 0.579 Eval Acc: 0.804 length of Dataset: 10000\n",
            "Epoch: 008 Train Loss: 0.536 Train Acc: 0.814 Eval Loss: 0.571 Eval Acc: 0.808 length of Dataset: 10000\n",
            "Epoch: 009 Train Loss: 0.505 Train Acc: 0.827 Eval Loss: 0.556 Eval Acc: 0.815 length of Dataset: 10000\n",
            "Epoch: 010 Train Loss: 0.474 Train Acc: 0.838 Eval Loss: 0.535 Eval Acc: 0.817 length of Dataset: 10000\n",
            "Epoch: 011 Train Loss: 0.450 Train Acc: 0.845 Eval Loss: 0.531 Eval Acc: 0.817 length of Dataset: 10000\n",
            "Epoch: 012 Train Loss: 0.428 Train Acc: 0.851 Eval Loss: 0.480 Eval Acc: 0.840 length of Dataset: 10000\n",
            "Epoch: 013 Train Loss: 0.413 Train Acc: 0.859 Eval Loss: 0.487 Eval Acc: 0.836 length of Dataset: 10000\n",
            "Epoch: 014 Train Loss: 0.394 Train Acc: 0.863 Eval Loss: 0.499 Eval Acc: 0.829 length of Dataset: 10000\n",
            "Epoch: 015 Train Loss: 0.377 Train Acc: 0.870 Eval Loss: 0.468 Eval Acc: 0.845 length of Dataset: 10000\n",
            "Epoch: 016 Train Loss: 0.367 Train Acc: 0.874 Eval Loss: 0.570 Eval Acc: 0.821 length of Dataset: 10000\n",
            "Epoch: 017 Train Loss: 0.360 Train Acc: 0.874 Eval Loss: 0.520 Eval Acc: 0.827 length of Dataset: 10000\n",
            "Epoch: 018 Train Loss: 0.342 Train Acc: 0.881 Eval Loss: 0.496 Eval Acc: 0.835 length of Dataset: 10000\n",
            "Epoch: 019 Train Loss: 0.337 Train Acc: 0.883 Eval Loss: 0.477 Eval Acc: 0.842 length of Dataset: 10000\n",
            "Epoch: 020 Train Loss: 0.324 Train Acc: 0.888 Eval Loss: 0.415 Eval Acc: 0.861 length of Dataset: 10000\n",
            "Epoch: 021 Train Loss: 0.311 Train Acc: 0.892 Eval Loss: 0.476 Eval Acc: 0.843 length of Dataset: 10000\n",
            "Epoch: 022 Train Loss: 0.312 Train Acc: 0.891 Eval Loss: 0.470 Eval Acc: 0.845 length of Dataset: 10000\n",
            "Epoch: 023 Train Loss: 0.299 Train Acc: 0.897 Eval Loss: 0.452 Eval Acc: 0.850 length of Dataset: 10000\n",
            "Epoch: 024 Train Loss: 0.297 Train Acc: 0.896 Eval Loss: 0.434 Eval Acc: 0.856 length of Dataset: 10000\n",
            "Epoch: 025 Train Loss: 0.288 Train Acc: 0.900 Eval Loss: 0.466 Eval Acc: 0.849 length of Dataset: 10000\n",
            "Epoch: 026 Train Loss: 0.281 Train Acc: 0.902 Eval Loss: 0.424 Eval Acc: 0.863 length of Dataset: 10000\n",
            "Epoch: 027 Train Loss: 0.275 Train Acc: 0.904 Eval Loss: 0.402 Eval Acc: 0.866 length of Dataset: 10000\n",
            "Epoch: 028 Train Loss: 0.273 Train Acc: 0.905 Eval Loss: 0.447 Eval Acc: 0.861 length of Dataset: 10000\n",
            "Epoch: 029 Train Loss: 0.271 Train Acc: 0.905 Eval Loss: 0.438 Eval Acc: 0.854 length of Dataset: 10000\n",
            "Epoch: 030 Train Loss: 0.260 Train Acc: 0.910 Eval Loss: 0.419 Eval Acc: 0.867 length of Dataset: 10000\n",
            "Epoch: 031 Train Loss: 0.261 Train Acc: 0.909 Eval Loss: 0.427 Eval Acc: 0.862 length of Dataset: 10000\n",
            "Epoch: 032 Train Loss: 0.256 Train Acc: 0.912 Eval Loss: 0.413 Eval Acc: 0.865 length of Dataset: 10000\n",
            "Epoch: 033 Train Loss: 0.252 Train Acc: 0.912 Eval Loss: 0.385 Eval Acc: 0.872 length of Dataset: 10000\n",
            "Epoch: 034 Train Loss: 0.247 Train Acc: 0.913 Eval Loss: 0.432 Eval Acc: 0.859 length of Dataset: 10000\n",
            "Epoch: 035 Train Loss: 0.244 Train Acc: 0.915 Eval Loss: 0.425 Eval Acc: 0.865 length of Dataset: 10000\n",
            "Epoch: 036 Train Loss: 0.241 Train Acc: 0.915 Eval Loss: 0.456 Eval Acc: 0.858 length of Dataset: 10000\n",
            "Epoch: 037 Train Loss: 0.239 Train Acc: 0.916 Eval Loss: 0.459 Eval Acc: 0.860 length of Dataset: 10000\n",
            "Epoch: 038 Train Loss: 0.233 Train Acc: 0.919 Eval Loss: 0.417 Eval Acc: 0.868 length of Dataset: 10000\n",
            "Epoch: 039 Train Loss: 0.236 Train Acc: 0.917 Eval Loss: 0.479 Eval Acc: 0.853 length of Dataset: 10000\n",
            "Epoch: 040 Train Loss: 0.229 Train Acc: 0.920 Eval Loss: 0.433 Eval Acc: 0.863 length of Dataset: 10000\n",
            "Epoch: 041 Train Loss: 0.227 Train Acc: 0.921 Eval Loss: 0.403 Eval Acc: 0.872 length of Dataset: 10000\n",
            "Epoch: 042 Train Loss: 0.225 Train Acc: 0.921 Eval Loss: 0.386 Eval Acc: 0.878 length of Dataset: 10000\n",
            "Epoch: 043 Train Loss: 0.217 Train Acc: 0.924 Eval Loss: 0.410 Eval Acc: 0.867 length of Dataset: 10000\n",
            "Epoch: 044 Train Loss: 0.225 Train Acc: 0.923 Eval Loss: 0.417 Eval Acc: 0.867 length of Dataset: 10000\n",
            "Epoch: 045 Train Loss: 0.217 Train Acc: 0.924 Eval Loss: 0.469 Eval Acc: 0.865 length of Dataset: 10000\n",
            "Epoch: 046 Train Loss: 0.216 Train Acc: 0.925 Eval Loss: 0.443 Eval Acc: 0.863 length of Dataset: 10000\n",
            "Epoch: 047 Train Loss: 0.214 Train Acc: 0.925 Eval Loss: 0.409 Eval Acc: 0.875 length of Dataset: 10000\n",
            "Epoch: 048 Train Loss: 0.213 Train Acc: 0.925 Eval Loss: 0.486 Eval Acc: 0.853 length of Dataset: 10000\n",
            "Epoch: 049 Train Loss: 0.211 Train Acc: 0.926 Eval Loss: 0.450 Eval Acc: 0.861 length of Dataset: 10000\n",
            "Epoch: 050 Train Loss: 0.206 Train Acc: 0.928 Eval Loss: 0.463 Eval Acc: 0.859 length of Dataset: 10000\n",
            "Epoch: 051 Train Loss: 0.203 Train Acc: 0.929 Eval Loss: 0.443 Eval Acc: 0.869 length of Dataset: 10000\n",
            "Epoch: 052 Train Loss: 0.203 Train Acc: 0.929 Eval Loss: 0.468 Eval Acc: 0.864 length of Dataset: 10000\n",
            "Epoch: 053 Train Loss: 0.204 Train Acc: 0.928 Eval Loss: 0.425 Eval Acc: 0.863 length of Dataset: 10000\n",
            "Epoch: 054 Train Loss: 0.204 Train Acc: 0.930 Eval Loss: 0.451 Eval Acc: 0.858 length of Dataset: 10000\n",
            "Epoch: 055 Train Loss: 0.202 Train Acc: 0.929 Eval Loss: 0.415 Eval Acc: 0.873 length of Dataset: 10000\n",
            "Epoch: 056 Train Loss: 0.198 Train Acc: 0.931 Eval Loss: 0.416 Eval Acc: 0.873 length of Dataset: 10000\n",
            "Epoch: 057 Train Loss: 0.195 Train Acc: 0.931 Eval Loss: 0.445 Eval Acc: 0.866 length of Dataset: 10000\n",
            "Epoch: 058 Train Loss: 0.198 Train Acc: 0.930 Eval Loss: 0.391 Eval Acc: 0.878 length of Dataset: 10000\n",
            "Epoch: 059 Train Loss: 0.197 Train Acc: 0.931 Eval Loss: 0.460 Eval Acc: 0.865 length of Dataset: 10000\n",
            "Epoch: 060 Train Loss: 0.191 Train Acc: 0.934 Eval Loss: 0.401 Eval Acc: 0.878 length of Dataset: 10000\n",
            "Epoch: 061 Train Loss: 0.195 Train Acc: 0.932 Eval Loss: 0.442 Eval Acc: 0.870 length of Dataset: 10000\n",
            "Epoch: 062 Train Loss: 0.197 Train Acc: 0.932 Eval Loss: 0.574 Eval Acc: 0.839 length of Dataset: 10000\n",
            "Epoch: 063 Train Loss: 0.199 Train Acc: 0.930 Eval Loss: 0.421 Eval Acc: 0.874 length of Dataset: 10000\n",
            "Epoch: 064 Train Loss: 0.187 Train Acc: 0.935 Eval Loss: 0.398 Eval Acc: 0.878 length of Dataset: 10000\n",
            "Epoch: 065 Train Loss: 0.184 Train Acc: 0.936 Eval Loss: 0.468 Eval Acc: 0.872 length of Dataset: 10000\n",
            "Epoch: 066 Train Loss: 0.189 Train Acc: 0.934 Eval Loss: 0.424 Eval Acc: 0.871 length of Dataset: 10000\n",
            "Epoch: 067 Train Loss: 0.183 Train Acc: 0.936 Eval Loss: 0.443 Eval Acc: 0.869 length of Dataset: 10000\n",
            "Epoch: 068 Train Loss: 0.185 Train Acc: 0.936 Eval Loss: 0.495 Eval Acc: 0.857 length of Dataset: 10000\n",
            "Epoch: 069 Train Loss: 0.185 Train Acc: 0.935 Eval Loss: 0.420 Eval Acc: 0.875 length of Dataset: 10000\n",
            "Epoch: 070 Train Loss: 0.186 Train Acc: 0.936 Eval Loss: 0.429 Eval Acc: 0.876 length of Dataset: 10000\n",
            "Epoch: 071 Train Loss: 0.180 Train Acc: 0.937 Eval Loss: 0.438 Eval Acc: 0.867 length of Dataset: 10000\n",
            "Epoch: 072 Train Loss: 0.186 Train Acc: 0.934 Eval Loss: 0.437 Eval Acc: 0.866 length of Dataset: 10000\n",
            "Epoch: 073 Train Loss: 0.179 Train Acc: 0.938 Eval Loss: 0.498 Eval Acc: 0.854 length of Dataset: 10000\n",
            "Epoch: 074 Train Loss: 0.180 Train Acc: 0.937 Eval Loss: 0.437 Eval Acc: 0.870 length of Dataset: 10000\n",
            "Epoch: 075 Train Loss: 0.181 Train Acc: 0.937 Eval Loss: 0.441 Eval Acc: 0.868 length of Dataset: 10000\n",
            "Epoch: 076 Train Loss: 0.177 Train Acc: 0.938 Eval Loss: 0.526 Eval Acc: 0.843 length of Dataset: 10000\n",
            "Epoch: 077 Train Loss: 0.180 Train Acc: 0.937 Eval Loss: 0.393 Eval Acc: 0.882 length of Dataset: 10000\n",
            "Epoch: 078 Train Loss: 0.178 Train Acc: 0.938 Eval Loss: 0.413 Eval Acc: 0.873 length of Dataset: 10000\n",
            "Epoch: 079 Train Loss: 0.174 Train Acc: 0.939 Eval Loss: 0.478 Eval Acc: 0.862 length of Dataset: 10000\n",
            "Epoch: 080 Train Loss: 0.177 Train Acc: 0.938 Eval Loss: 0.465 Eval Acc: 0.861 length of Dataset: 10000\n",
            "Epoch: 081 Train Loss: 0.177 Train Acc: 0.938 Eval Loss: 0.463 Eval Acc: 0.867 length of Dataset: 10000\n",
            "Epoch: 082 Train Loss: 0.175 Train Acc: 0.938 Eval Loss: 0.427 Eval Acc: 0.873 length of Dataset: 10000\n",
            "Epoch: 083 Train Loss: 0.180 Train Acc: 0.937 Eval Loss: 0.430 Eval Acc: 0.872 length of Dataset: 10000\n",
            "Epoch: 084 Train Loss: 0.174 Train Acc: 0.939 Eval Loss: 0.459 Eval Acc: 0.858 length of Dataset: 10000\n",
            "Epoch: 085 Train Loss: 0.172 Train Acc: 0.941 Eval Loss: 0.435 Eval Acc: 0.876 length of Dataset: 10000\n",
            "Epoch: 086 Train Loss: 0.172 Train Acc: 0.939 Eval Loss: 0.495 Eval Acc: 0.859 length of Dataset: 10000\n",
            "Epoch: 087 Train Loss: 0.174 Train Acc: 0.938 Eval Loss: 0.401 Eval Acc: 0.885 length of Dataset: 10000\n",
            "Epoch: 088 Train Loss: 0.169 Train Acc: 0.942 Eval Loss: 0.425 Eval Acc: 0.874 length of Dataset: 10000\n",
            "Epoch: 089 Train Loss: 0.172 Train Acc: 0.939 Eval Loss: 0.476 Eval Acc: 0.862 length of Dataset: 10000\n",
            "Epoch: 090 Train Loss: 0.166 Train Acc: 0.942 Eval Loss: 0.422 Eval Acc: 0.870 length of Dataset: 10000\n",
            "Epoch: 091 Train Loss: 0.178 Train Acc: 0.938 Eval Loss: 0.417 Eval Acc: 0.875 length of Dataset: 10000\n",
            "Epoch: 092 Train Loss: 0.167 Train Acc: 0.942 Eval Loss: 0.378 Eval Acc: 0.888 length of Dataset: 10000\n",
            "Epoch: 093 Train Loss: 0.166 Train Acc: 0.943 Eval Loss: 0.397 Eval Acc: 0.879 length of Dataset: 10000\n",
            "Epoch: 094 Train Loss: 0.169 Train Acc: 0.941 Eval Loss: 0.402 Eval Acc: 0.879 length of Dataset: 10000\n",
            "Epoch: 095 Train Loss: 0.162 Train Acc: 0.942 Eval Loss: 0.416 Eval Acc: 0.877 length of Dataset: 10000\n",
            "Epoch: 096 Train Loss: 0.162 Train Acc: 0.944 Eval Loss: 0.431 Eval Acc: 0.876 length of Dataset: 10000\n",
            "Epoch: 097 Train Loss: 0.169 Train Acc: 0.941 Eval Loss: 0.438 Eval Acc: 0.869 length of Dataset: 10000\n",
            "Epoch: 098 Train Loss: 0.167 Train Acc: 0.942 Eval Loss: 0.436 Eval Acc: 0.871 length of Dataset: 10000\n",
            "Epoch: 099 Train Loss: 0.168 Train Acc: 0.941 Eval Loss: 0.471 Eval Acc: 0.864 length of Dataset: 10000\n",
            "model Architecture Printing...\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "Resnet Pretrained Model Printing...\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "Resnet Pretrained and Fused Model Printing...\n",
            "ResNet(\n",
            "  (conv1): ConvReLU2d(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (bn1): Identity()\n",
            "  (relu): Identity()\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu): Identity()\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu): Identity()\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu): Identity()\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu): Identity()\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu): Identity()\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu): Identity()\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu): Identity()\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu): Identity()\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "Checking Model Equivalence ...\n",
            "Printing Quantization Configuration...\n",
            "QConfig(activation=functools.partial(<class 'torch.quantization.observer.HistogramObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric))\n",
            "prepare Quantization Aware Training...\n",
            "Training QAT Model...\n",
            "Epoch: 000 Train Loss: 0.087 Train Acc: 0.970 Eval Loss: 0.329 Eval Acc: 0.908 length of Dataset: 10000\n",
            "Epoch: 001 Train Loss: 0.063 Train Acc: 0.978 Eval Loss: 0.329 Eval Acc: 0.911 length of Dataset: 10000\n",
            "Epoch: 002 Train Loss: 0.054 Train Acc: 0.981 Eval Loss: 0.333 Eval Acc: 0.914 length of Dataset: 10000\n",
            "Epoch: 003 Train Loss: 0.049 Train Acc: 0.982 Eval Loss: 0.333 Eval Acc: 0.913 length of Dataset: 10000\n",
            "Epoch: 004 Train Loss: 0.043 Train Acc: 0.985 Eval Loss: 0.337 Eval Acc: 0.914 length of Dataset: 10000\n",
            "Epoch: 005 Train Loss: 0.037 Train Acc: 0.987 Eval Loss: 0.354 Eval Acc: 0.913 length of Dataset: 10000\n",
            "Epoch: 006 Train Loss: 0.038 Train Acc: 0.987 Eval Loss: 0.356 Eval Acc: 0.915 length of Dataset: 10000\n",
            "Epoch: 007 Train Loss: 0.034 Train Acc: 0.988 Eval Loss: 0.363 Eval Acc: 0.915 length of Dataset: 10000\n",
            "Epoch: 008 Train Loss: 0.034 Train Acc: 0.988 Eval Loss: 0.372 Eval Acc: 0.915 length of Dataset: 10000\n",
            "Epoch: 009 Train Loss: 0.030 Train Acc: 0.989 Eval Loss: 0.377 Eval Acc: 0.917 length of Dataset: 10000\n",
            "Completed Quantization Aware Training!\n",
            "Convert Quantized Model...\n",
            "QuantizedResNet18(\n",
            "  (quant): Quantize(scale=tensor([0.0374]), zero_point=tensor([57]), dtype=torch.quint8)\n",
            "  (dequant): DeQuantize()\n",
            "  (model_fp32): ResNet(\n",
            "    (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.07318175584077835, zero_point=0, padding=(1, 1))\n",
            "    (bn1): Identity()\n",
            "    (relu): Identity()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.03437134623527527, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu): Identity()\n",
            "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.08969853818416595, zero_point=80, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.125263974070549, zero_point=45\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): QuantizedReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.037605978548526764, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu): Identity()\n",
            "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.08174358308315277, zero_point=88, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.09743516892194748, zero_point=50\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): QuantizedReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.028006814420223236, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu): Identity()\n",
            "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.0711292251944542, zero_point=61, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.05630742385983467, zero_point=56)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.07517227530479431, zero_point=66\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): QuantizedReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.027222810313105583, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu): Identity()\n",
            "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.07354143261909485, zero_point=65, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.09452217817306519, zero_point=49\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): QuantizedReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.022543201223015785, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu): Identity()\n",
            "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.06665214151144028, zero_point=58, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.029218293726444244, zero_point=61)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.07662250101566315, zero_point=60\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): QuantizedReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.02208409272134304, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu): Identity()\n",
            "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.03791861981153488, zero_point=72, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.06971517950296402, zero_point=42\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): QuantizedReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.012706060893833637, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu): Identity()\n",
            "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.034657809883356094, zero_point=54, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.050378814339637756, zero_point=55)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.08590470254421234, zero_point=46\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): QuantizedReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.004058675840497017, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu): Identity()\n",
            "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.007797203026711941, zero_point=62, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.055983275175094604, zero_point=9\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): QuantizedReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantizedLinear(in_features=512, out_features=10, scale=0.3266929090023041, zero_point=35, qscheme=torch.per_channel_affine)\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "evaluate Pretrained Model and QAT Model\n",
            "FP32 evaluation accuracy: 0.864\n",
            "INT8 evaluation accuracy: 0.916\n",
            "FP32 CPU Inference Latency: 1044.51 ms / sample\n",
            "INT8 CPU Inference Latency: 356.56 ms / sample\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44.78 MB\n",
            "11.31 MB\n"
          ]
        }
      ],
      "source": [
        "print(\"%.2f MB\" %(os.path.getsize(\"saved_models/resnet18_cifar10.pt\")/1e6))\n",
        "print(\"%.2f MB\" %(os.path.getsize(\"saved_models/resnet18_quantized_cifar10.pt\")/1e6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Image inference 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RecursiveScriptModule(\n",
              "  original_name=QuantizedResNet18\n",
              "  (quant): RecursiveScriptModule(original_name=Quantize)\n",
              "  (dequant): RecursiveScriptModule(original_name=DeQuantize)\n",
              "  (model_fp32): RecursiveScriptModule(\n",
              "    original_name=ResNet\n",
              "    (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
              "    (bn1): RecursiveScriptModule(original_name=Identity)\n",
              "    (relu): RecursiveScriptModule(original_name=Identity)\n",
              "    (maxpool): RecursiveScriptModule(original_name=MaxPool2d)\n",
              "    (layer1): RecursiveScriptModule(\n",
              "      original_name=Sequential\n",
              "      (0): RecursiveScriptModule(\n",
              "        original_name=BasicBlock\n",
              "        (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
              "        (bn1): RecursiveScriptModule(original_name=Identity)\n",
              "        (relu): RecursiveScriptModule(original_name=Identity)\n",
              "        (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
              "        (bn2): RecursiveScriptModule(original_name=Identity)\n",
              "        (skip_add): RecursiveScriptModule(\n",
              "          original_name=QFunctional\n",
              "          (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
              "        )\n",
              "        (relu2): RecursiveScriptModule(original_name=ReLU)\n",
              "      )\n",
              "      (1): RecursiveScriptModule(\n",
              "        original_name=BasicBlock\n",
              "        (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
              "        (bn1): RecursiveScriptModule(original_name=Identity)\n",
              "        (relu): RecursiveScriptModule(original_name=Identity)\n",
              "        (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
              "        (bn2): RecursiveScriptModule(original_name=Identity)\n",
              "        (skip_add): RecursiveScriptModule(\n",
              "          original_name=QFunctional\n",
              "          (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
              "        )\n",
              "        (relu2): RecursiveScriptModule(original_name=ReLU)\n",
              "      )\n",
              "    )\n",
              "    (layer2): RecursiveScriptModule(\n",
              "      original_name=Sequential\n",
              "      (0): RecursiveScriptModule(\n",
              "        original_name=BasicBlock\n",
              "        (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
              "        (bn1): RecursiveScriptModule(original_name=Identity)\n",
              "        (relu): RecursiveScriptModule(original_name=Identity)\n",
              "        (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
              "        (bn2): RecursiveScriptModule(original_name=Identity)\n",
              "        (downsample): RecursiveScriptModule(\n",
              "          original_name=Sequential\n",
              "          (0): RecursiveScriptModule(original_name=Conv2d)\n",
              "          (1): RecursiveScriptModule(original_name=Identity)\n",
              "        )\n",
              "        (skip_add): RecursiveScriptModule(\n",
              "          original_name=QFunctional\n",
              "          (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
              "        )\n",
              "        (relu2): RecursiveScriptModule(original_name=ReLU)\n",
              "      )\n",
              "      (1): RecursiveScriptModule(\n",
              "        original_name=BasicBlock\n",
              "        (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
              "        (bn1): RecursiveScriptModule(original_name=Identity)\n",
              "        (relu): RecursiveScriptModule(original_name=Identity)\n",
              "        (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
              "        (bn2): RecursiveScriptModule(original_name=Identity)\n",
              "        (skip_add): RecursiveScriptModule(\n",
              "          original_name=QFunctional\n",
              "          (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
              "        )\n",
              "        (relu2): RecursiveScriptModule(original_name=ReLU)\n",
              "      )\n",
              "    )\n",
              "    (layer3): RecursiveScriptModule(\n",
              "      original_name=Sequential\n",
              "      (0): RecursiveScriptModule(\n",
              "        original_name=BasicBlock\n",
              "        (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
              "        (bn1): RecursiveScriptModule(original_name=Identity)\n",
              "        (relu): RecursiveScriptModule(original_name=Identity)\n",
              "        (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
              "        (bn2): RecursiveScriptModule(original_name=Identity)\n",
              "        (downsample): RecursiveScriptModule(\n",
              "          original_name=Sequential\n",
              "          (0): RecursiveScriptModule(original_name=Conv2d)\n",
              "          (1): RecursiveScriptModule(original_name=Identity)\n",
              "        )\n",
              "        (skip_add): RecursiveScriptModule(\n",
              "          original_name=QFunctional\n",
              "          (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
              "        )\n",
              "        (relu2): RecursiveScriptModule(original_name=ReLU)\n",
              "      )\n",
              "      (1): RecursiveScriptModule(\n",
              "        original_name=BasicBlock\n",
              "        (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
              "        (bn1): RecursiveScriptModule(original_name=Identity)\n",
              "        (relu): RecursiveScriptModule(original_name=Identity)\n",
              "        (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
              "        (bn2): RecursiveScriptModule(original_name=Identity)\n",
              "        (skip_add): RecursiveScriptModule(\n",
              "          original_name=QFunctional\n",
              "          (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
              "        )\n",
              "        (relu2): RecursiveScriptModule(original_name=ReLU)\n",
              "      )\n",
              "    )\n",
              "    (layer4): RecursiveScriptModule(\n",
              "      original_name=Sequential\n",
              "      (0): RecursiveScriptModule(\n",
              "        original_name=BasicBlock\n",
              "        (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
              "        (bn1): RecursiveScriptModule(original_name=Identity)\n",
              "        (relu): RecursiveScriptModule(original_name=Identity)\n",
              "        (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
              "        (bn2): RecursiveScriptModule(original_name=Identity)\n",
              "        (downsample): RecursiveScriptModule(\n",
              "          original_name=Sequential\n",
              "          (0): RecursiveScriptModule(original_name=Conv2d)\n",
              "          (1): RecursiveScriptModule(original_name=Identity)\n",
              "        )\n",
              "        (skip_add): RecursiveScriptModule(\n",
              "          original_name=QFunctional\n",
              "          (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
              "        )\n",
              "        (relu2): RecursiveScriptModule(original_name=ReLU)\n",
              "      )\n",
              "      (1): RecursiveScriptModule(\n",
              "        original_name=BasicBlock\n",
              "        (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
              "        (bn1): RecursiveScriptModule(original_name=Identity)\n",
              "        (relu): RecursiveScriptModule(original_name=Identity)\n",
              "        (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
              "        (bn2): RecursiveScriptModule(original_name=Identity)\n",
              "        (skip_add): RecursiveScriptModule(\n",
              "          original_name=QFunctional\n",
              "          (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
              "        )\n",
              "        (relu2): RecursiveScriptModule(original_name=ReLU)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): RecursiveScriptModule(original_name=AdaptiveAvgPool2d)\n",
              "    (fc): RecursiveScriptModule(\n",
              "      original_name=Linear\n",
              "      (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_fp32 = torch.load('saved_models/resnet18_cifar10_Full.pth')\n",
        "# model_fp32.load_state_dict(torch.load('resnet18.pt'))\n",
        "model_int8 = torch.jit.load('saved_models/resnet18_quantized_cifar10.pt')\n",
        "\n",
        "cuda_device = torch.device(\"cuda:0\")\n",
        "cpu_device = torch.device(\"cpu:0\")\n",
        "\n",
        "model_fp32.to(cpu_device)\n",
        "model_int8.to(cpu_device)\n",
        "\n",
        "model_fp32.eval()\n",
        "model_int8.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "input_image = Image.open(\"cat.jpg\")\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.CenterCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
        "])\n",
        "\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_notQuantized = model_fp32(input_batch)\n",
        "output_Quantized = model_int8(input_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.7825, -1.0212, -0.5345,  5.7240, -0.5737,  0.3027, -0.4872, -0.7152,\n",
            "        -0.9683, -0.9441], grad_fn=<SelectBackward>)\n",
            "tensor([-4.9004, -6.8606,  0.0000, 21.2350, -0.9801,  4.5737,  0.3267, -4.9004,\n",
            "        -3.5936, -4.9004])\n"
          ]
        }
      ],
      "source": [
        "# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
        "print(output_notQuantized[0])\n",
        "print(output_Quantized[0])\n",
        "\n",
        "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
        "probabilities_notQuantized = torch.nn.functional.softmax(output_notQuantized[0], dim=0)\n",
        "probabilities_Quantized = torch.nn.functional.softmax(output_Quantized[0], dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "not Quantized Model's Perfomance:\n",
            "cat 0.9833205342292786\n",
            "dog 0.004347770940512419\n",
            "frog 0.001973461825400591\n",
            "bird 0.00188222317956388\n",
            "deer 0.00180993159301579\n",
            "\n",
            "Quantized Model's Perfomance:\n",
            "cat 1.0\n",
            "dog 5.808613678937036e-08\n",
            "frog 8.310375165798689e-10\n",
            "bird 5.994318930113707e-10\n",
            "deer 2.2495552642887162e-10\n"
          ]
        }
      ],
      "source": [
        "categories = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "\n",
        "\n",
        "# Show top categories per image\n",
        "top5_prob_notQuantized, top5_catid_notQuantized = torch.topk(probabilities_notQuantized, 5)\n",
        "top5_prob_Quantized, top5_catid_Quantized = torch.topk(probabilities_Quantized, 5)\n",
        "\n",
        "print(\"not Quantized Model's Perfomance:\")\n",
        "for i in range(top5_prob_notQuantized.size(0)):\n",
        "    print(categories[top5_catid_notQuantized[i]], top5_prob_notQuantized[i].item())\n",
        "    \n",
        "print(\"\\nQuantized Model's Perfomance:\")    \n",
        "for i in range(top5_prob_Quantized.size(0)):\n",
        "    print(categories[top5_catid_Quantized[i]], top5_prob_Quantized[i].item())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "QAT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ca7d112caef4dbe87dedf7627847e08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17e90a5325d94f4881066712971e77bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c5bd5c9be36428bb379e76d2a4e7041",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9726b7986894d5ab030657741cc72c8",
            "value": 170498071
          }
        },
        "227676e5ba1542d9848216dbecdfc1d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f3174f697ce4ffdaa400b5397175caf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e25c33aebdd54a32ab372276da851a65",
              "IPY_MODEL_17e90a5325d94f4881066712971e77bd",
              "IPY_MODEL_8cad15043d024aa8930530311b50d54b"
            ],
            "layout": "IPY_MODEL_227676e5ba1542d9848216dbecdfc1d7"
          }
        },
        "7c5bd5c9be36428bb379e76d2a4e7041": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cad15043d024aa8930530311b50d54b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f1a2c4dc3a341c184512e75ad4043f9",
            "placeholder": "​",
            "style": "IPY_MODEL_f17a0fbc1d1c406da612c6364f375e9b",
            "value": " 170499072/? [00:03&lt;00:00, 52297403.27it/s]"
          }
        },
        "8f1a2c4dc3a341c184512e75ad4043f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9726b7986894d5ab030657741cc72c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e25c33aebdd54a32ab372276da851a65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ca7d112caef4dbe87dedf7627847e08",
            "placeholder": "​",
            "style": "IPY_MODEL_e88bdbda8cce46f985fd7a2a28fa99db",
            "value": ""
          }
        },
        "e88bdbda8cce46f985fd7a2a28fa99db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f17a0fbc1d1c406da612c6364f375e9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
