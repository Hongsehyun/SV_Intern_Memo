{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torchvision\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import datasets\n",
    "# import torchvision.transforms as transforms\n",
    "# import os\n",
    "# import time\n",
    "# import sys\n",
    "# import torch.quantization\n",
    "\n",
    "# # # warnings 설정\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\n",
    "#     action='ignore',\n",
    "#     category=DeprecationWarning,\n",
    "#     module=r'.*'\n",
    "# )\n",
    "# warnings.filterwarnings(\n",
    "#     action='default',\n",
    "#     module=r'torch.quantization'\n",
    "# )\n",
    "\n",
    "# # 반복 가능한 결과를 위한 랜덤 시드 지정하기\n",
    "# torch.manual_seed(191009)\n",
    "\n",
    "# from torch.quantization import QuantStub, DeQuantStub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "# def _make_divisible(v, divisor, min_value=None):\n",
    "#     \"\"\"\n",
    "#     이 함수는 원본 TensorFlow 저장소에서 가져왔습니다.\n",
    "#     모든 계층이 8로 나누어지는 채널 숫자를 가지고 있습니다.\n",
    "#     이곳에서 확인 가능합니다:\n",
    "#     https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "#     :param v:\n",
    "#     :param divisor:\n",
    "#     :param min_value:\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "#     if min_value is None:\n",
    "#         min_value = divisor\n",
    "#     new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "#     # 내림은 10% 넘게 내려가지 않는 것을 보장합니다.\n",
    "#     if new_v < 0.9 * v:\n",
    "#         new_v += divisor\n",
    "#     return new_v\n",
    "\n",
    "\n",
    "# class ConvBNReLU(nn.Sequential):\n",
    "#     def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n",
    "#         padding = (kernel_size - 1) // 2\n",
    "#         super(ConvBNReLU, self).__init__(\n",
    "#             nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
    "#             nn.BatchNorm2d(out_planes, momentum=0.1),\n",
    "#             # ReLU로 교체\n",
    "#             nn.ReLU(inplace=False)\n",
    "#         )\n",
    "\n",
    "\n",
    "# class InvertedResidual(nn.Module):\n",
    "#     def __init__(self, inp, oup, stride, expand_ratio):\n",
    "#         super(InvertedResidual, self).__init__()\n",
    "#         self.stride = stride\n",
    "#         assert stride in [1, 2]\n",
    "\n",
    "#         hidden_dim = int(round(inp * expand_ratio))\n",
    "#         self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "#         layers = []\n",
    "#         if expand_ratio != 1:\n",
    "#             # pw\n",
    "#             layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n",
    "#         layers.extend([\n",
    "#             # dw\n",
    "#             ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n",
    "#             # pw-linear\n",
    "#             nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "#             nn.BatchNorm2d(oup, momentum=0.1),\n",
    "#         ])\n",
    "#         self.conv = nn.Sequential(*layers)\n",
    "#         # torch.add를 floatfunctional로 교체\n",
    "#         self.skip_add = nn.quantized.FloatFunctional()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if self.use_res_connect:\n",
    "#             return self.skip_add.add(x, self.conv(x))\n",
    "#         else:\n",
    "#             return self.conv(x)\n",
    "\n",
    "\n",
    "# class MobileNetV2(nn.Module):\n",
    "#     def __init__(self, num_classes=1000, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n",
    "#         \"\"\"\n",
    "#         MobileNet V2 메인 클래스\n",
    "#         Args:\n",
    "#             num_classes (int): 클래스 숫자\n",
    "#             width_mult (float): 넓이 multiplier - 이 수를 통해 각 계층의 채널 개수를 조절\n",
    "#             inverted_residual_setting: 네트워크 구조\n",
    "#             round_nearest (int): 각 계층의 채널 숫를 이 숫자의 배수로 반올림\n",
    "#             1로 설정하면 반올림 정지\n",
    "#         \"\"\"\n",
    "#         super(MobileNetV2, self).__init__()\n",
    "#         block = InvertedResidual\n",
    "#         input_channel = 32\n",
    "#         last_channel = 1280\n",
    "\n",
    "#         if inverted_residual_setting is None:\n",
    "#             inverted_residual_setting = [\n",
    "#                 # t, c, n, s\n",
    "#                 [1, 16, 1, 1],\n",
    "#                 [6, 24, 2, 2],\n",
    "#                 [6, 32, 3, 2],\n",
    "#                 [6, 64, 4, 2],\n",
    "#                 [6, 96, 3, 1],\n",
    "#                 [6, 160, 3, 2],\n",
    "#                 [6, 320, 1, 1],\n",
    "#             ]\n",
    "\n",
    "#         # 사용자가 t,c,n,s를 필요하다는 것을 안다는 전제하에 첫 번째 요소만 확인\n",
    "#         if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
    "#             raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
    "#                              \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
    "\n",
    "#         # 첫 번째 계층 만들기\n",
    "#         input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
    "#         self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
    "#         features = [ConvBNReLU(3, input_channel, stride=2)]\n",
    "#         # 역전된 잔차 블럭(inverted residual blocks) 만들기\n",
    "#         for t, c, n, s in inverted_residual_setting:\n",
    "#             output_channel = _make_divisible(c * width_mult, round_nearest)\n",
    "#             for i in range(n):\n",
    "#                 stride = s if i == 0 else 1\n",
    "#                 features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n",
    "#                 input_channel = output_channel\n",
    "#         # 마지막 계층들 만들기\n",
    "#         features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
    "#         # nn.Sequential로 만들기\n",
    "#         self.features = nn.Sequential(*features)\n",
    "#         self.quant = QuantStub()\n",
    "#         self.dequant = DeQuantStub()\n",
    "#         # 분류기(classifier) 만들기\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(self.last_channel, num_classes),\n",
    "#         )\n",
    "\n",
    "#         # 가중치 초기화\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.zeros_(m.bias)\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 nn.init.ones_(m.weight)\n",
    "#                 nn.init.zeros_(m.bias)\n",
    "#             elif isinstance(m, nn.Linear):\n",
    "#                 nn.init.normal_(m.weight, 0, 0.01)\n",
    "#                 nn.init.zeros_(m.bias)\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         x = self.quant(x)\n",
    "\n",
    "#         x = self.features(x)\n",
    "#         x = x.mean([2, 3])\n",
    "#         x = self.classifier(x)\n",
    "#         x = self.dequant(x)\n",
    "#         return x\n",
    "\n",
    "#     # 양자화 전에 Conv+BN과 Conv+BN+Relu 모듈 결합(fusion)\n",
    "#     # 이 연산은 숫자를 변경하지 않음\n",
    "#     def fuse_model(self):\n",
    "#         for m in self.modules():\n",
    "#             if type(m) == ConvBNReLU:\n",
    "#                 torch.quantization.fuse_modules(m, ['0', '1', '2'], inplace=True)\n",
    "#             if type(m) == InvertedResidual:\n",
    "#                 for idx in range(len(m.conv)):\n",
    "#                     if type(m.conv[idx]) == nn.Conv2d:\n",
    "#                         torch.quantization.fuse_modules(m.conv, [str(idx), str(idx + 1)], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AverageMeter(object):\n",
    "#     \"\"\"평균과 현재 값 계산 및 저장\"\"\"\n",
    "#     def __init__(self, name, fmt=':f'):\n",
    "#         self.name = name\n",
    "#         self.fmt = fmt\n",
    "#         self.reset()\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.val = 0\n",
    "#         self.avg = 0\n",
    "#         self.sum = 0\n",
    "#         self.count = 0\n",
    "\n",
    "#     def update(self, val, n=1):\n",
    "#         self.val = val\n",
    "#         self.sum += val * n\n",
    "#         self.count += n\n",
    "#         self.avg = self.sum / self.count\n",
    "\n",
    "#     def __str__(self):\n",
    "#         fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "#         return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "# def accuracy(output, target, topk=(1,)):\n",
    "#     \"\"\"특정 k값을 위해 top k 예측의 정확도 계산\"\"\"\n",
    "#     with torch.no_grad():\n",
    "#         maxk = max(topk)\n",
    "#         batch_size = target.size(0)\n",
    "\n",
    "#         _, pred = output.topk(maxk, 1, True, True)\n",
    "#         pred = pred.t()\n",
    "#         correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "#         res = []\n",
    "#         for k in topk:\n",
    "#             correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "#             res.append(correct_k.mul_(100.0 / batch_size))\n",
    "#         return res\n",
    "\n",
    "\n",
    "# def evaluate(model, criterion, data_loader, neval_batches):\n",
    "#     model.eval()\n",
    "#     top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "#     top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "#     cnt = 0\n",
    "#     with torch.no_grad():\n",
    "#         for image, target in data_loader:\n",
    "#             output = model(image)\n",
    "#             loss = criterion(output, target)\n",
    "#             cnt += 1\n",
    "#             acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "#             print('.', end = '')\n",
    "#             top1.update(acc1[0], image.size(0))\n",
    "#             top5.update(acc5[0], image.size(0))\n",
    "#             if cnt >= neval_batches:\n",
    "#                  return top1, top5\n",
    "\n",
    "#     return top1, top5\n",
    "\n",
    "# def load_model(model_file):\n",
    "#     model = MobileNetV2()\n",
    "#     state_dict = torch.load(model_file)\n",
    "#     model.load_state_dict(state_dict)\n",
    "#     model.to('cpu')\n",
    "#     return model\n",
    "\n",
    "# def print_size_of_model(model):\n",
    "#     torch.save(model.state_dict(), \"temp.p\")\n",
    "#     print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "#     os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_data_loaders(data_path):\n",
    "\n",
    "#     normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225])\n",
    "#     dataset = torchvision.datasets.ImageNet(\n",
    "#            data_path, split=\"train\",\n",
    "#          transforms.Compose([\n",
    "#                    transforms.RandomResizedCrop(224),\n",
    "#                    transforms.RandomHorizontalFlip(),\n",
    "#                    transforms.ToTensor(),\n",
    "#                    normalize,\n",
    "#                ]))\n",
    "#     dataset_test = torchvision.datasets.ImageNet(\n",
    "#           data_path, split=\"val\",\n",
    "#               transforms.Compose([\n",
    "#                   transforms.Resize(256),\n",
    "#                   transforms.CenterCrop(224),\n",
    "#                   transforms.ToTensor(),\n",
    "#                   normalize,\n",
    "#               ]))\n",
    "\n",
    "#     train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "#     test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "#     data_loader = torch.utils.data.DataLoader(\n",
    "#         dataset, batch_size=train_batch_size,\n",
    "#         sampler=train_sampler)\n",
    "\n",
    "#     data_loader_test = torch.utils.data.DataLoader(\n",
    "#         dataset_test, batch_size=eval_batch_size,\n",
    "#         sampler=test_sampler)\n",
    "\n",
    "#     return data_loader, data_loader_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import torch.quantization\n",
    "\n",
    "# # warnings 설정\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=DeprecationWarning,\n",
    "    module=r'.*'\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    action='default',\n",
    "    module=r'torch.quantization'\n",
    ")\n",
    "\n",
    "# 반복 가능한 결과를 위한 랜덤 시드 지정하기\n",
    "torch.manual_seed(191009)\n",
    "\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "from model import FasterRCNNVGG16\n",
    "from model import faster_rcnn\n",
    "from trainer import FasterRCNNTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"평균과 현재 값 계산 및 저장\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"특정 k값을 위해 top k 예측의 정확도 계산\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_loader, neval_batches):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            print('.', end = '')\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "            if cnt >= neval_batches:\n",
    "                 return top1, top5\n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "def load_model(model_file):\n",
    "    # model = MobileNetV2()\n",
    "    model = FasterRCNNVGG16()\n",
    "    model.load_state_dict(torch.load(model_file), strict=False)\n",
    "    model.to('cpu')\n",
    "    return model\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, data_loader, device, ntrain_batches):\n",
    "    model.train()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    avgloss = AverageMeter('Loss', '1.5f')\n",
    "\n",
    "    cnt = 0\n",
    "    for image, target in data_loader:\n",
    "        start_time = time.time()\n",
    "        print('.', end = '')\n",
    "        cnt += 1\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        output = model(image)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        top1.update(acc1[0], image.size(0))\n",
    "        top5.update(acc5[0], image.size(0))\n",
    "        avgloss.update(loss, image.size(0))\n",
    "        if cnt >= ntrain_batches:\n",
    "            print('Loss', avgloss.avg)\n",
    "\n",
    "            print('Training: * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "                  .format(top1=top1, top5=top5))\n",
    "            return\n",
    "\n",
    "    print('Full imagenet train set:  * Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Inverted Residual Block: Before fusion \n",
      "\n",
      " Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      " Inverted Residual Block: After fusion\n",
      "\n",
      " Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "# data_path = '~/.data/imagenet'\n",
    "saved_model_dir = 'checkpoints/'\n",
    "float_model_file = 'tmp.pt'\n",
    "scripted_float_model_file = 'fasterrcnn_quantization_scripted.pt'\n",
    "scripted_quantized_model_file = 'fasterrcnn_quantization_scripted_quantized.pt'\n",
    "\n",
    "# train_batch_size = 30\n",
    "# eval_batch_size = 50\n",
    "train_batch_size = 1\n",
    "eval_batch_size = 1\n",
    "\n",
    "# data_loader, data_loader_test = prepare_data_loaders(data_path)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "float_model = load_model(saved_model_dir + float_model_file).to('cpu')\n",
    "\n",
    "# 다음으로 \"모듈 결합\"을 합니다. 모듈 결합은 메모리 접근을 줄여 모델을 빠르게 만들면서\n",
    "# 정확도 수치를 향상시킵니다. 모듈 결합은 어떠한 모델에라도 사용할 수 있지만,\n",
    "# 양자화된 모델에 사용하는 것이 특히나 더 일반적입니다.\n",
    "# print('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\n",
    "print('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.extractor[0])\n",
    "float_model.eval()\n",
    "\n",
    "# 모듈 결합\n",
    "float_model.fuse_model()\n",
    "\n",
    "# Conv+BN+Relu와 Conv+Relu 결합에 유의\n",
    "# print('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)\n",
    "print('\\n Inverted Residual Block: After fusion\\n\\n',float_model.extractor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FasterRCNNVGG16' object has no attribute 'Sequential'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8264/1501751996.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodules_to_fuse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'extractor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rpn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'head'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mqat_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuse_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqat_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Sequential'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# qat_model.fuse_model()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/ao/quantization/fuse_modules.py\u001b[0m in \u001b[0;36mfuse_modules\u001b[0;34m(model, modules_to_fuse, inplace, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodule_element\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules_to_fuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# Handle case of modules_to_fuse being a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0m_fuse_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules_to_fuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuser_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuse_custom_config_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# Handle case of modules_to_fuse being a list of lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/ao/quantization/fuse_modules.py\u001b[0m in \u001b[0;36m_fuse_modules\u001b[0;34m(model, modules_to_fuse, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mmod_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules_to_fuse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mmod_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Fuse list of modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/ao/quantization/fuse_modules.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(model, submodule_key)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mcur_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mcur_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcur_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1186\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FasterRCNNVGG16' object has no attribute 'Sequential'"
     ]
    }
   ],
   "source": [
    "qat_model = load_model(saved_model_dir + float_model_file)\n",
    "\n",
    "modules_to_fuse = [['extractor', 'rpn', 'head']]\n",
    "qat_model = torch.quantization.fuse_modules(qat_model, ['Sequential'])\n",
    "# qat_model.fuse_model()\n",
    "\n",
    "# optimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n",
    "# qat_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New New New Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/ao/quantization/observer.py:177: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'training_loop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14852/565675550.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# run the training loop (not shown)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fp32_prepared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Convert the observed model to a quantized model. This does several things:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_loop' is not defined"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# # define a floating point model where some layers could benefit from QAT\n",
    "# class M(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(M, self).__init__()\n",
    "#         # QuantStub converts tensors from floating point to quantized\n",
    "#         self.quant = torch.quantization.QuantStub()\n",
    "#         self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "#         self.bn = torch.nn.BatchNorm2d(1)\n",
    "#         self.relu = torch.nn.ReLU()\n",
    "#         # DeQuantStub converts tensors from quantized to floating point\n",
    "#         self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.quant(x)\n",
    "#         x = self.conv(x)\n",
    "#         x = self.bn(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.dequant(x)\n",
    "#         return x\n",
    "\n",
    "# # create a model instance\n",
    "# model_fp32 = M()\n",
    "\n",
    "# # model must be set to train mode for QAT logic to work\n",
    "# model_fp32.train()\n",
    "\n",
    "# # attach a global qconfig, which contains information about what kind\n",
    "# # of observers to attach. Use 'fbgemm' for server inference and\n",
    "# # 'qnnpack' for mobile inference. Other quantization configurations such\n",
    "# # as selecting symmetric or assymetric quantization and MinMax or L2Norm\n",
    "# # calibration techniques can be specified here.\n",
    "# model_fp32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "\n",
    "# # fuse the activations to preceding layers, where applicable\n",
    "# # this needs to be done manually depending on the model architecture\n",
    "# model_fp32_fused = torch.quantization.fuse_modules(model_fp32,\n",
    "#     [['conv', 'bn', 'relu']])\n",
    "\n",
    "# # Prepare the model for QAT. This inserts observers and fake_quants in\n",
    "# # the model that will observe weight and activation tensors during calibration.\n",
    "# model_fp32_prepared = torch.quantization.prepare_qat(model_fp32_fused)\n",
    "\n",
    "# # run the training loop (not shown)\n",
    "# training_loop(model_fp32_prepared)\n",
    "\n",
    "# # Convert the observed model to a quantized model. This does several things:\n",
    "# # quantizes the weights, computes and stores the scale and bias value to be\n",
    "# # used with each activation tensor, fuses modules where appropriate,\n",
    "# # and replaces key operators with quantized implementations.\n",
    "# model_fp32_prepared.eval()\n",
    "# model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# # run the model, relevant calculations will happen in int8\n",
    "# res = model_int8(input_fp32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
